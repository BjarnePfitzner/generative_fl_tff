# @package _global_

defaults:
  - override /model: vae
  - override /dataset: mnist
  - override /evaluation: development
  - override /differential_privacy: local
  - override /training/vae_loss_fn: kl_loss
  - override /training/early_stopping:
      - accuracy
      - fid

experiment_name: vae_sync_ed_mnist

training:
  total_rounds: 200
  aggregation_step_optimizer:
    type: SGD
    lr: 1.0
  client_optimizer:
    type: Adam

evaluation:
  run_fid_eval: True
  plot_l2_norms: True
  rounds_per_eval: 10

model:
  type: sync_ed
  latent_dim: 32
  module: small_ccvae


hydra:
  sweeper:
    params:
      differential_privacy.l2_norm_clip:
        - 0.05
        - 0.1
      differential_privacy.noise_multiplier:
        - 0.8
        - 0.9
        - 1.0
        - 1.25
        - 1.5
        - 1.7
      training.batch_size:
        - 30
        - 40
        - 60
      training.local_epochs:
        - 1
        - 2
        - 5
        - 10
      training.clients_per_round:
        - 0.01
        - 0.02
        - 0.05
        - 0.1
      training.client_optimizer.lr:
        - 0.0001
        - 0.0002
        - 0.0005
        - 0.001
        - 0.002
      training.aggregation_step_optimizer.momentum:
        - 0.0
        - 0.5
        - 0.9
      training.vae_loss_fn.kl_beta:
        - 1.0
        - 10.0
      training.vae_loss_fn.kl_beta_warmup:
        - 0
        - 10
