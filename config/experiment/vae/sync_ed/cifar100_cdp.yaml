# @package _global_

defaults:
  - override /model: vae
  - override /dataset: cifar100
  - override /evaluation: development
  - override /differential_privacy: central
  - override /training/vae_loss_fn: kl_loss
  - override /training/early_stopping:
      - fid

experiment_name: vae_sync_ed_cifar100

training:
  total_rounds: 100
  aggregation_step_optimizer:
    type: SGD
    lr: 1.0
  client_optimizer:
    type: Adam
model:
  type: sync_ed
  latent_dim: 128

evaluation:
  run_fid_eval: True
  rounds_per_eval: 20


hydra:
  sweeper:
    params:
      differential_privacy.l2_norm_clip:
        - 0.1
        - 0.2
      differential_privacy.noise_multiplier:
        - 0.6
        - 0.7
        - 0.8
        - 0.9
        - 1.0
        - 1.1
        - 1.2
        - 1.5
        - 2.0
      differential_privacy.adaptive:
        - True
        - False
      model.module:
        - cifar_ccvae
        - soft_intro_res_vae
      training.batch_size:
        - 16
        - 32
        - 64
      training.client_optimizer.lr:
        - 0.0001
        - 0.0002
        - 0.0005
        - 0.001
      training.aggregation_step_optimizer.momentum:
        - 0.0
        - 0.5
        - 0.9
        - 0.99
      training.local_epochs:
        - 1
        - 5
        - 10
      training.clients_per_round:
        - 0.1
        - 0.15
        - 0.2
        - 0.25
      training.vae_loss_fn.kl_beta:
        - 0.1
        - 1.0
        - 5.0
        - 10.0
      training.vae_loss_fn.kl_beta_warmup:
        - 0
        - 10
        - 100